{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('titanic.csv')\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Taking a close look at the missing values\n",
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = data.isnull().sum()/data.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets explore throught the data \n",
    "# We focus on gender a little too much so we will be comparing correlations for sub classes in gender \n",
    "# lets take a look at the correlation of age with survival probability\n",
    "survived = 'survived'\n",
    "not_survived = 'not survived'\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
    "women = data[data['Gender']=='female']\n",
    "men = data[data['Gender']=='male']\n",
    "ax = sns.histplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False,alpha=0.3)\n",
    "ax = sns.histplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
    "ax.legend()\n",
    "ax.set_title('Female')\n",
    "ax = sns.histplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False,alpha=0.3)\n",
    "ax = sns.histplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\n",
    "ax.legend()\n",
    "_ = ax.set_title('Male')\n",
    "# its found that different age groups in men and women have varying survival chances it is not a linear correlation \n",
    "# so we need to make classes out of age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look for any correlation between embarked and survival chance \n",
    "FacetGrid = sns.FacetGrid(data, row='Embarked', aspect=1.6)\n",
    "FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Gender', palette='dark:#1f77b4',  order=None, hue_order=None )\n",
    "FacetGrid.add_legend()\n",
    "\n",
    "# Women on port Q have a higher chance of survival but women on ports S and C have low chances of survival\n",
    "# Men have a high survival probability if they are on port C and S, but a low probability if they are on port Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pclass have a negative correlation with survived so we look into it a bit deeper \n",
    "sns.barplot(x='Pclass', y='Survived', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pclass = 1 implies the person is more likely to survive and other pclasses people in a localised age clusters survived \n",
    "grid = sns.FacetGrid(data, col='Survived', row='Pclass', aspect=1.6)\n",
    "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we combine sibsp and parch to create a feature called relatives\n",
    "data['relatives'] = data['SibSp'] + data['Parch']\n",
    "data.loc[data['relatives'] > 0, 'not_alone'] = 0\n",
    "data.loc[data['relatives'] == 0, 'not_alone'] = 1\n",
    "data['not_alone'] = data['not_alone'].astype(int)\n",
    "data['not_alone'].value_counts()\n",
    "axes = sns.catplot(data=data, x='relatives',y='Survived', aspect = 2.5,kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As 'cabin' has a lot of null values we drop the column\n",
    "data = data.drop(columns=['Cabin'])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets fill up the nan values in age with near realitic values \n",
    "mean = data[\"Age\"].mean()\n",
    "std = data[\"Age\"].std()\n",
    "is_null = data[\"Age\"].isnull().sum()\n",
    "# compute random numbers between the mean, std and is_null\n",
    "rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "# fill NaN values in Age column with random values generated\n",
    "age_slice = data[\"Age\"].copy()\n",
    "age_slice[np.isnan(age_slice)] = rand_age\n",
    "data[\"Age\"] = age_slice\n",
    "data[\"Age\"] = data[\"Age\"].astype(int)\n",
    "\n",
    "data[\"Age\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with only 2 missing values we can fill embarked with most common value\n",
    "data['Embarked'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Embarked'] = data['Embarked'].fillna('S')\n",
    "# all the null values are handled now \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping gender to numerical values\n",
    "data['Gender'] = data['Gender'].map({'male': 0, 'female': 1})  # Convert categorical to numerical\n",
    "print(data['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'] = data['Age'].astype(int)\n",
    "data.loc[ data['Age'] <= 11, 'Age'] = 0\n",
    "data.loc[(data['Age'] > 11) & (data['Age'] <= 18), 'Age'] = 1\n",
    "data.loc[(data['Age'] > 18) & (data['Age'] <= 22), 'Age'] = 2\n",
    "data.loc[(data['Age'] > 22) & (data['Age'] <= 27), 'Age'] = 3\n",
    "data.loc[(data['Age'] > 27) & (data['Age'] <= 33), 'Age'] = 4\n",
    "data.loc[(data['Age'] > 33) & (data['Age'] <= 40), 'Age'] = 5\n",
    "data.loc[(data['Age'] > 40) & (data['Age'] <= 66), 'Age'] = 6\n",
    "data.loc[ data['Age'] > 66, 'Age'] = 6\n",
    "\n",
    "# its a nice distribution \n",
    "data['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "# extract titles\n",
    "data['Title'] = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "# replace titles with a more common title or as Rare\n",
    "data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\n",
    "                                        'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "data['Title'] = data['Title'].replace('Mlle', 'Miss')\n",
    "data['Title'] = data['Title'].replace('Ms', 'Miss')\n",
    "data['Title'] = data['Title'].replace('Mme', 'Mrs')\n",
    "# convert titles into numbers\n",
    "data['Title'] = data['Title'].map(titles)\n",
    "# filling NaN with 0, to get safe\n",
    "data['Title'] = data['Title'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(columns = ['Ticket','PassengerId','Name','SibSp','Parch'])\n",
    "data = data.drop(columns = ['Ticket','PassengerId','Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_data = pd.get_dummies(data,columns=['Pclass','Embarked'],drop_first=True)\n",
    "ohe_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat =  ohe_data.corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_mat,annot=True,cmap=\"coolwarm\",linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and target\n",
    "X = ohe_data.drop(columns=['Survived'], axis=1)\n",
    "y = ohe_data['Survived']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomForestClassifier with default parameters\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Default Random Forest Classifier Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "n_estimators= [5,10 ,25 , 50, 100, 200]\n",
    "max_depth= [None, 5, 10, 15,25,50]\n",
    "min_samples_split= [2, 5, 10,20,50]\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "\n",
    "for estimator in n_estimators:\n",
    "    for maxdepth in max_depth:\n",
    "        for split in min_samples_split:\n",
    "            rfc = RandomForestClassifier(n_estimators=estimator,max_depth=maxdepth,min_samples_split=split,random_state=42)\n",
    "            rfc.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = rfc.predict(X_test)\n",
    "\n",
    "            # Evaluate the model\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            if estimator==10 and maxdepth == 50:\n",
    "                accuracy.append(acc)\n",
    "                print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(min_samples_split,accuracy)\n",
    "plt.xlabel('Min Samples Split')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(matrix,annot=True,fmt='.0f',cmap=\"coolwarm\",linewidths=.5)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel(\"Predicted\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rfc.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DecisionTree with default parameters\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Default Decision Tree Classifier Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(matrix,annot=True,fmt='.0f',cmap=\"coolwarm\",linewidths=.5)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel(\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "    'max_depth': [None, 2, 4, 6, 8, 10],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4, 6],  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': [None, 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_dtree = grid_search.best_estimator_\n",
    "y_pred = best_dtree.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(matrix,annot=True,fmt='.0f',cmap=\"coolwarm\",linewidths=.5)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel(\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the model\n",
    "log_model = LogisticRegression(max_iter=200)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log = log_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "\n",
    "solvers = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "max_iters = [10,50,100,150,200,250,300,500]\n",
    "\n",
    "for solver in solvers:\n",
    "    for max_iter in max_iters:\n",
    "        model = LogisticRegression(solver=solver,max_iter=max_iter)\n",
    "        model.fit(X_train,y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        print(f\"solver = {solver}  ------- iter = {max_iter}\")\n",
    "        print(classification_report(y_pred=y_pred,y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "    'solver': ['liblinear', 'saga'],      # Algorithms to use for optimization\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'], # Regularization type\n",
    "    'max_iter': [100, 200, 300]            # Maximum number of iterations\n",
    "}\n",
    "\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=log_model, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "best_log_model = grid_search.best_estimator_\n",
    "y_pred = best_log_model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_before_tuning = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy before tuning:\", accuracy_before_tuning)\n",
    "print(\"\\nClassification Report (Before Tuning):\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid for KNN\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 21)),  # Test n_neighbors from 1 to 20\n",
    "    'weights': ['uniform', 'distance']    # Test uniform vs distance weights\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred_tuned = best_knn.predict(X_test)\n",
    "\n",
    "# Print accuracy and classification report after tuning\n",
    "accuracy_after_tuning = accuracy_score(y_test, y_pred_tuned)\n",
    "print(\"Accuracy after tuning:\", accuracy_after_tuning)\n",
    "print(\"\\nClassification Report (After Tuning):\")\n",
    "print(classification_report(y_test, y_pred_tuned))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
